#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CSV, JSON, XML, Excel, Parquet —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞,
—É—Å—Ç–æ–π—á–∏–≤—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π.
–í–µ—Ä—Å–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –¥–ª—è CSV.
"""

import os
import json
import sys
import argparse
import logging
import textwrap
import time
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple, Optional, Union
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –í–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
REQUIRED_PACKAGES = {
    'pandas': 'pip install pandas',
    'lxml': 'pip install lxml', 
    'ollama': 'pip install ollama'
}

OPTIONAL_PACKAGES = {
    'openpyxl': 'pip install openpyxl',
    'pyarrow': 'pip install pyarrow',
    'rich': 'pip install rich'
}

def check_and_import_required():
    missing = []
    for package, install_cmd in REQUIRED_PACKAGES.items():
        try:
            globals()[package] = __import__(package)
        except ImportError:
            missing.append(f"{package}: {install_cmd}")
    
    if missing:
        print("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã:")
        for pkg in missing:
            print(f"  {pkg}")
        sys.exit(1)

def check_and_import_optional():
    available = {}
    for package in OPTIONAL_PACKAGES.keys():
        try:
            available[package] = __import__(package)
        except ImportError:
            available[package] = None
    return available

check_and_import_required()
optional_packages = check_and_import_optional()

import pandas as pd
import subprocess
import csv
from lxml import etree

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    import ollama
    OLLAMA_CLIENT_OK = True
except ImportError:
    OLLAMA_CLIENT_OK = False

try:
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.table import Table
    from rich.panel import Panel
    RICH_OK = True
except ImportError:
    RICH_OK = False

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DEFAULT_INPUT_DIR = "input"
DEFAULT_OUTPUT_DIR = "output"
SUPPORTED_EXTS = {".csv", ".tsv", ".json", ".xml", ".xlsx", ".xls", ".parquet"}
DEFAULT_MODEL = "qwen3:30b"

class DateTimeJSONEncoder(json.JSONEncoder):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π JSON encoder –¥–ª—è datetime –æ–±—ä–µ–∫—Ç–æ–≤"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        if hasattr(obj, '__dict__'):
            return obj.__dict__
        return super().default(obj)

def safe_json_dumps(obj, **kwargs):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π datetime"""
    return json.dumps(obj, cls=DateTimeJSONEncoder, **kwargs)

@dataclass
class FileInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ"""
    path: str
    format: str
    size_bytes: int
    modified: datetime
    
    def to_dict(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å —Å —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–º–∏ —Ç–∏–ø–∞–º–∏"""
        return {
            "path": self.path,
            "format": self.format,
            "size_bytes": self.size_bytes,
            "modified": self.modified.isoformat(),
            "size_mb": round(self.size_bytes / 1024 / 1024, 2)
        }
        
@dataclass  
class AnalysisConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    input_dir: str
    output_dir: Optional[str]
    model_name: str
    sample_rows: int
    max_chars: int
    expected_cols: Optional[int]
    file_pattern: Optional[str]
    save_results: bool
    verbose: bool
    force_separator: Optional[str] = None  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å

@dataclass
class ColumnInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–ª–±—Ü–µ"""
    name: str
    dtype: str
    non_null_count: int
    null_count: int
    null_percentage: float
    unique_count: int
    example_values: List[Any]
    
    def to_dict(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            "name": self.name,
            "dtype": self.dtype,
            "non_null_count": self.non_null_count,
            "null_count": self.null_count,
            "null_percentage": round(self.null_percentage, 2),
            "unique_count": self.unique_count,
            "example_values": [str(v) for v in self.example_values]  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–∞–º
        }
        
@dataclass
class DataOverview:
    """–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
    file_info: FileInfo
    data_type: str
    rows: int
    cols: int
    columns: List[ColumnInfo]
    memory_usage_mb: float
    separator_info: Optional[Dict[str, Any]] = None
    read_attempts: Optional[List[Tuple[Dict, str]]] = None
    
    def to_dict(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å"""
        result = {
            "file_info": self.file_info.to_dict(),
            "data_type": self.data_type,
            "rows": self.rows,
            "cols": self.cols,
            "columns": [col.to_dict() for col in self.columns],
            "memory_usage_mb": round(self.memory_usage_mb, 2)
        }
        if self.separator_info:
            result["separator_info"] = self.separator_info
        if self.read_attempts:
            result["read_attempts"] = self.read_attempts
        return result

class Logger:
    """–ü—Ä–æ—Å—Ç–æ–π –ª–æ–≥–≥–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π rich"""
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.console = Console() if RICH_OK else None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
        logging.basicConfig(
            level=logging.DEBUG if verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler(sys.stderr)]
        )
        self.logger = logging.getLogger(__name__)
    
    def info(self, message: str):
        if self.console:
            self.console.print(f"[blue]‚Ñπ[/blue] {message}")
        else:
            print(f"‚Ñπ {message}")
        self.logger.info(message)
    
    def success(self, message: str):
        if self.console:
            self.console.print(f"[green]‚úì[/green] {message}")
        else:
            print(f"‚úì {message}")
        self.logger.info(message)
    
    def warning(self, message: str):
        if self.console:
            self.console.print(f"[yellow]‚ö†[/yellow] {message}")
        else:
            print(f"‚ö† {message}")
        self.logger.warning(message)
    
    def error(self, message: str):
        if self.console:
            self.console.print(f"[red]‚úó[/red] {message}")
        else:
            print(f"‚úó {message}")
        self.logger.error(message)
        
    def debug(self, message: str):
        if self.verbose:
            if self.console:
                self.console.print(f"[dim]Debug: {message}[/dim]")
            else:
                print(f"Debug: {message}")
        self.logger.debug(message)

class FileHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, logger: Logger):
        self.logger = logger
    
    def find_files(self, input_dir: str, pattern: Optional[str] = None) -> List[FileInfo]:
        """–ù–∞–π—Ç–∏ –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã"""
        if not os.path.isdir(input_dir):
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {input_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        files = []
        for path in Path(input_dir).rglob("*"):
            if path.is_file() and path.suffix.lower() in SUPPORTED_EXTS:
                if pattern and pattern not in path.name:
                    continue
                
                stat = path.stat()
                files.append(FileInfo(
                    path=str(path),
                    format=self.detect_format(str(path)),
                    size_bytes=stat.st_size,
                    modified=datetime.fromtimestamp(stat.st_mtime)
                ))
        
        return sorted(files, key=lambda f: f.modified, reverse=True)
    
    def detect_format(self, path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞"""
        ext = Path(path).suffix.lower()
        format_map = {
            ".csv": "csv",
            ".tsv": "tsv", 
            ".json": "json",
            ".xml": "xml",
            ".xlsx": "excel",
            ".xls": "excel",
            ".parquet": "parquet"
        }
        return format_map.get(ext, "unknown")

class CSVReader:
    """–£—Å—Ç–æ–π—á–∏–≤—ã–π —á–∏—Ç–∞—Ç–µ–ª—å CSV —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è"""
    
    def __init__(self, logger: Logger):
        self.logger = logger
    
    def detect_separator(self, path: str, sample_lines: int = 10, encoding: str = 'utf-8') -> Tuple[str, Dict[str, Any]]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è CSV"""
        separators = [';', ',', '\t', '|']  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ—á–∫–µ —Å –∑–∞–ø—è—Ç–æ–π
        separator_scores = {}
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        encodings_to_try = [encoding, 'utf-8', 'cp1251', 'iso-8859-1']
        sample_content = []
        used_encoding = encoding
        
        for enc in encodings_to_try:
            try:
                with open(path, 'r', encoding=enc, errors='replace') as f:
                    sample_content = []
                    for i, line in enumerate(f):
                        if i >= sample_lines:
                            break
                        sample_content.append(line.strip())
                used_encoding = enc
                break
            except Exception as e:
                self.logger.debug(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞ {enc} –Ω–µ –ø–æ–¥–æ—à–ª–∞: {e}")
                continue
        
        if not sample_content:
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –Ω–∏ –≤ –æ–¥–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ")
            return ',', {"error": "encoding_failed", "encoding": used_encoding}
        
        self.logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞: {used_encoding}")
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
        for sep in separators:
            counts = []
            for line in sample_content:
                if line.strip():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    counts.append(line.count(sep))
            
            if not counts:
                continue
                
            # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
            max_count = max(counts)
            if max_count > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π)
                most_common_count = max(set(counts), key=counts.count)
                consistency = counts.count(most_common_count) / len(counts)
                
                # –°—Ä–µ–¥–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫
                avg_columns = most_common_count + 1
                
                # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–ª–æ–Ω–æ–∫
                score = consistency * avg_columns
                
                separator_scores[sep] = {
                    'score': score,
                    'consistency': consistency,
                    'avg_columns': avg_columns,
                    'counts': counts,
                    'most_common_count': most_common_count
                }
        
        if not separator_scores:
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º ','")
            return ',', {"error": "no_separator_detected", "encoding": used_encoding}
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        best_sep = max(separator_scores.keys(), key=lambda x: separator_scores[x]['score'])
        best_info = separator_scores[best_sep]
        
        # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–µ—Ç–µ–∫—Ü–∏–∏
        detection_info = {
            "detected_separator": best_sep,
            "separator_name": self._get_separator_name(best_sep),
            "expected_columns": int(best_info['avg_columns']),
            "consistency": round(best_info['consistency'], 3),
            "score": round(best_info['score'], 3),
            "encoding": used_encoding,
            "all_separators": {
                self._get_separator_name(sep): {
                    "score": round(info['score'], 3),
                    "columns": int(info['avg_columns']),
                    "consistency": round(info['consistency'], 3)
                }
                for sep, info in separator_scores.items()
            }
        }
        
        self.logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{self._get_separator_name(best_sep)}' ({best_sep}) - –æ–∂–∏–¥–∞–µ—Ç—Å—è {best_info['avg_columns']} –∫–æ–ª–æ–Ω–æ–∫")
        
        return best_sep, detection_info
    
    def _get_separator_name(self, sep: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è"""
        names = {
            ',': '–∑–∞–ø—è—Ç–∞—è',
            ';': '—Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π',
            '\t': '—Ç–∞–±—É–ª—è—Ü–∏—è',
            '|': '–≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è —á–µ—Ä—Ç–∞'
        }
        return names.get(sep, f"'{sep}'")
    
    def diagnose_csv_structure(self, path: str, lines_to_check: int = 5):
        """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV —Ñ–∞–π–ª–∞"""
        self.logger.info("=== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê CSV ===")
        
        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        encodings = ['utf-8', 'cp1251', 'iso-8859-1']
        lines = []
        used_encoding = 'utf-8'
        
        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding, errors='replace') as f:
                    lines = [f.readline().strip() for _ in range(lines_to_check)]
                used_encoding = encoding
                break
            except:
                continue
        
        print(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {used_encoding}")
        print(f"–ü–µ—Ä–≤—ã–µ {len(lines)} —Å—Ç—Ä–æ–∫ —Ñ–∞–π–ª–∞:")
        for i, line in enumerate(lines[:3]):
            print(f"  {i+1}: {line[:150]}{'...' if len(line) > 150 else ''}")
        
        print("\n–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π:")
        separators = [',', ';', '\t', '|']
        for sep in separators:
            sep_name = self._get_separator_name(sep)
            counts = [line.count(sep) for line in lines if line.strip()]
            if counts:
                max_count = max(counts)
                avg_count = sum(counts) / len(counts)
                consistency = counts.count(max_count) / len(counts) if max_count > 0 else 0
                print(f"  {sep_name}: –º–∞–∫—Å={max_count}, —Å—Ä–µ–¥–Ω–µ–µ={avg_count:.1f}, –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å={consistency:.1f}")
        print()
    
    def robust_read_csv(self, path: str, expected_cols: Optional[int] = None, 
                       prefer_seps: Optional[Tuple[str, ...]] = None,
                       force_separator: Optional[str] = None) -> Tuple[pd.DataFrame, List[Tuple[Dict, str]], Dict[str, Any]]:
        """–£—Å—Ç–æ–π—á–∏–≤–æ–µ —á—Ç–µ–Ω–∏–µ CSV —Å –ø–µ—Ä–µ–±–æ—Ä–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        attempts = []
        separator_info = {}
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        if force_separator:
            prefer_seps = (force_separator,)
            separator_info = {"forced_separator": force_separator}
            self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{self._get_separator_name(force_separator)}'")
        elif prefer_seps is None:
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
            detected_sep, separator_info = self.detect_separator(path)
            prefer_seps = (detected_sep, ";", ",", "\t", "|")
            self.logger.debug(f"–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–ª–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{detected_sep}'")
        
        configs = self._generate_configs(prefer_seps)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen = set()
        unique_configs = []
        for cfg in configs:
            key = tuple(sorted(cfg.items()))
            if key not in seen:
                seen.add(key)
                unique_configs.append(cfg)
        
        self.logger.debug(f"–ü–æ–ø—ã—Ç–æ–∫ —á—Ç–µ–Ω–∏—è CSV: {len(unique_configs)}")
        
        last_err = None
        best_df = None
        best_attempts = []
        best_cols_count = 0
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        for i, cfg in enumerate(unique_configs[:25]):
            try:
                self.logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {i+1}: {cfg}")
                df = pd.read_csv(path, **cfg)
                
                attempt_info = (cfg, f"OK: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
                attempts.append(attempt_info)
                
                # –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                is_better = False
                
                # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω–æ –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤
                if expected_cols:
                    if df.shape[1] == expected_cols:
                        self.logger.success(f"CSV –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –æ–∂–∏–¥–∞–µ–º—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç–æ–ª–±—Ü–æ–≤: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
                        return df, attempts, separator_info
                    elif df.shape[1] > best_cols_count and df.shape[1] <= expected_cols * 1.5:
                        is_better = True
                else:
                    # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–µ —Å—Ç–æ–ª–±—Ü–æ–≤ (–æ–±—ã—á–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä)
                    if df.shape[1] > best_cols_count:
                        is_better = True
                
                if is_better:
                    best_df = df
                    best_attempts = attempts.copy()
                    best_cols_count = df.shape[1]
                
                # –ï—Å–ª–∏ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–∂–∏–¥–∞–Ω–∏–π
                if not expected_cols and df.shape[1] >= 5:
                    self.logger.success(f"CSV –ø—Ä–æ—á–∏—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
                    return df, attempts, separator_info
                    
            except Exception as e:
                last_err = e
                error_msg = str(e)[:200].replace('\n', ' ')
                attempts.append((cfg, f"ERR: {error_msg}"))
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if best_df is not None:
            if best_cols_count > 1:
                self.logger.success(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_df.shape[0]} —Å—Ç—Ä–æ–∫, {best_cols_count} —Å—Ç–æ–ª–±—Ü–æ–≤")
            else:
                self.logger.warning(f"–ù–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç —Å {best_cols_count} —Å—Ç–æ–ª–±—Ü–æ–º. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º.")
            return best_df, best_attempts, separator_info
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
        error_msg = f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —á—Ç–µ–Ω–∏—è CSV –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_err}"
        self.logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    def _generate_configs(self, prefer_seps: Tuple[str, ...]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —á—Ç–µ–Ω–∏—è CSV"""
        configs = []
        
        for sep in prefer_seps:
            # –ü—Ä–æ—Å—Ç—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å (–±—ã—Å—Ç—Ä–µ–µ)
            configs.extend([
                {"sep": sep},  # C engine - —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π
                {"sep": sep, "skipinitialspace": True},
                {"engine": "python", "sep": sep},
                {"engine": "python", "sep": sep, "quotechar": '"'},
                {"engine": "python", "sep": sep, "quotechar": '"', "skipinitialspace": True},
            ])
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            configs.extend([
                {"engine": "python", "sep": sep, "on_bad_lines": "skip"},
                {"engine": "python", "sep": sep, "quotechar": '"', "escapechar": "\\"},
                {"engine": "python", "sep": sep, "quotechar": '"', "escapechar": "\\", "on_bad_lines": "skip"},
                {"engine": "python", "sep": sep, "encoding": "utf-8-sig"},
                {"engine": "python", "sep": sep, "encoding": "cp1251"},
                {"engine": "python", "sep": sep, "decimal": ",", "on_bad_lines": "skip"},
                {"engine": "python", "sep": sep, "thousands": " ", "on_bad_lines": "skip"},
            ])
        
        return configs

class DataAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.logger = Logger(config.verbose)
        self.file_handler = FileHandler(self.logger)
        self.csv_reader = CSVReader(self.logger)
    
    def analyze_file(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        self.logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_info.path} ({file_info.format})")
        
        try:
            if file_info.format in ["csv", "tsv"]:
                return self._analyze_csv(file_info)
            elif file_info.format == "json":
                return self._analyze_json(file_info)
            elif file_info.format == "xml":
                return self._analyze_xml(file_info)
            elif file_info.format == "excel":
                return self._analyze_excel(file_info)
            elif file_info.format == "parquet":
                return self._analyze_parquet(file_info)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_info.format}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_info.path}: {e}")
            raise
    
    def _analyze_csv(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ CSV —Ñ–∞–π–ª–∞"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –≤ verbose —Ä–µ–∂–∏–º–µ
        if self.config.verbose:
            self.csv_reader.diagnose_csv_structure(file_info.path)
        
        df = None
        attempts = []
        separator_info = {}
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ø–æ—Å–æ–± (–±—ã—Å—Ç—Ä–æ)
        try:
            df = pd.read_csv(file_info.path)
            attempts = [({"method": "standard_pandas"}, f"OK: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ 1 —Å—Ç–æ–ª–±–µ—Ü, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
            if df.shape[1] == 1:
                first_value = str(df.iloc[0, 0]) if len(df) > 0 else ""
                if ';' in first_value or '\t' in first_value:
                    self.logger.warning("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –¥–∞–ª–æ 1 —Å—Ç–æ–ª–±–µ—Ü, –Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏. –ü—Ä–æ–±—É–µ–º —É—Å—Ç–æ–π—á–∏–≤–æ–µ —á—Ç–µ–Ω–∏–µ.")
                    df = None  # –°–±—Ä–æ—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–º—É —á—Ç–µ–Ω–∏—é
                
        except Exception as e:
            attempts = [({"method": "standard_pandas"}, f"FAILED: {str(e)[:100]}")]
            self.logger.debug(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ: {e}")
        
        # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –ò–õ–ò –¥–∞–ª–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if df is None or df.shape[1] <= 1:
            self.logger.info("–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–º—É —á—Ç–µ–Ω–∏—é —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            prefer_seps = None
            if file_info.format == "tsv":
                prefer_seps = ("\t", ";", ",", "|")
            
            # –£—Å—Ç–æ–π—á–∏–≤–æ–µ —á—Ç–µ–Ω–∏–µ
            df, robust_attempts, separator_info = self.csv_reader.robust_read_csv(
                file_info.path, 
                expected_cols=self.config.expected_cols,
                prefer_seps=prefer_seps,
                force_separator=self.config.force_separator
            )
            attempts.extend(robust_attempts)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±–∑–æ—Ä
        overview = self._create_dataframe_overview(df, file_info)
        overview.separator_info = separator_info
        overview.read_attempts = attempts
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ sample –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
        sample_df = df.head(self.config.sample_rows)
        sample = []
        
        for _, row in sample_df.iterrows():
            row_dict = {}
            for col, val in row.items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                if pd.isna(val):
                    row_dict[str(col)] = None
                elif isinstance(val, (pd.Timestamp, datetime)):
                    row_dict[str(col)] = val.isoformat() if hasattr(val, 'isoformat') else str(val)
                else:
                    row_dict[str(col)] = str(val)
            sample.append(row_dict)
        
        return {
            "overview": overview.to_dict(),
            "sample": sample,
            "statistics": self._get_dataframe_statistics(df)
        }
    
    def _analyze_json(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ JSON —Ñ–∞–π–ª–∞"""
        with open(file_info.path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        def summarize_structure(obj, depth=0, max_depth=3):
            if depth > max_depth:
                return "...depth limit..."
            if isinstance(obj, dict):
                return {k: summarize_structure(v, depth+1, max_depth) 
                       for k, v in list(obj.items())[:20]}
            elif isinstance(obj, list):
                return [summarize_structure(v, depth+1, max_depth) 
                       for v in obj[:10]]
            return type(obj).__name__
        
        # –ü–æ–ø—ã—Ç–∫–∞ —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        table_overview = None
        if isinstance(data, list) and all(isinstance(x, dict) for x in data[:100] if x):
            df = pd.DataFrame(data)
            table_overview = self._create_dataframe_overview(df, file_info)
            sample = df.head(self.config.sample_rows).to_dict(orient="records")
        else:
            sample = data[:self.config.sample_rows] if isinstance(data, list) else data
        
        overview = {
            "file_info": file_info.to_dict(),
            "data_type": "json",
            "root_type": type(data).__name__,
            "structure_preview": summarize_structure(data)
        }
        
        if table_overview:
            overview["table_like"] = table_overview.to_dict()
        
        return {"overview": overview, "sample": sample}
    
    def _analyze_xml(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ XML —Ñ–∞–π–ª–∞"""
        tree = etree.parse(file_info.path)
        root = tree.getroot()
        
        def element_signature(elem):
            return {
                "tag": elem.tag,
                "attributes": sorted(elem.attrib.keys()),
                "children_tags": sorted({child.tag for child in elem}),
                "has_text": bool((elem.text or "").strip())
            }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–≥–∞–º
        tag_counts = {}
        for elem in root.iter():
            tag_counts[elem.tag] = tag_counts.get(elem.tag, 0) + 1
        
        # –ü—Ä–∏–º–µ—Ä—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        samples = []
        for i, elem in enumerate(root.iter()):
            if i >= self.config.sample_rows:
                break
            samples.append({
                "tag": elem.tag,
                "signature": element_signature(elem),
                "text_sample": (elem.text or "").strip()[:200]
            })
        
        overview = {
            "file_info": file_info.to_dict(),
            "data_type": "xml",
            "root_tag": root.tag,
            "unique_tags": len(tag_counts),
            "tag_counts_top": sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20],
            "total_elements": sum(tag_counts.values())
        }
        
        return {"overview": overview, "sample": samples}
    
    def _analyze_excel(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ Excel —Ñ–∞–π–ª–∞"""
        if not optional_packages.get('openpyxl'):
            raise RuntimeError("–î–ª—è —Ä–∞–±–æ—Ç—ã —Å Excel –Ω—É–∂–µ–Ω openpyxl: pip install openpyxl")
        
        # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã
        xl_file = pd.ExcelFile(file_info.path)
        sheets_info = {}
        
        for sheet_name in xl_file.sheet_names:
            df = pd.read_excel(file_info.path, sheet_name=sheet_name)
            sheets_info[sheet_name] = {
                "overview": self._create_dataframe_overview(df, file_info).to_dict(),
                "sample": df.head(self.config.sample_rows).to_dict(orient="records")
            }
        
        overview = {
            "file_info": file_info.to_dict(),
            "data_type": "excel", 
            "sheets_count": len(xl_file.sheet_names),
            "sheet_names": xl_file.sheet_names,
            "sheets": sheets_info
        }
        
        return {"overview": overview, "sample": None}
    
    def _analyze_parquet(self, file_info: FileInfo) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ Parquet —Ñ–∞–π–ª–∞"""
        if not optional_packages.get('pyarrow'):
            raise RuntimeError("–î–ª—è —Ä–∞–±–æ—Ç—ã —Å Parquet –Ω—É–∂–µ–Ω pyarrow: pip install pyarrow")
        
        df = pd.read_parquet(file_info.path)
        overview = self._create_dataframe_overview(df, file_info)
        sample = df.head(self.config.sample_rows).to_dict(orient="records")
        
        return {
            "overview": overview.to_dict(),
            "sample": sample,
            "statistics": self._get_dataframe_statistics(df)
        }
    
    def _create_dataframe_overview(self, df: pd.DataFrame, file_info: FileInfo) -> DataOverview:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–∑–æ—Ä–∞ –¥–ª—è DataFrame"""
        columns = []
        for col in df.columns:
            series = df[col]
            null_count = series.isnull().sum()
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–π
            example_values = []
            non_null_series = series.dropna()
            for val in non_null_series.head(5):
                if isinstance(val, (pd.Timestamp, datetime)):
                    example_values.append(val.isoformat() if hasattr(val, 'isoformat') else str(val))
                else:
                    example_values.append(val)
            
            columns.append(ColumnInfo(
                name=str(col),
                dtype=str(series.dtype),
                non_null_count=int(series.count()),
                null_count=int(null_count),
                null_percentage=float(null_count / len(df) * 100),
                unique_count=int(series.nunique()),
                example_values=example_values
            ))
        
        return DataOverview(
            file_info=file_info,
            data_type="tabular",
            rows=int(len(df)),
            cols=int(len(df.columns)),
            columns=columns,
            memory_usage_mb=float(df.memory_usage(deep=True).sum() / 1024 / 1024)
        )
    
    def _get_dataframe_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ DataFrame"""
        stats = {
            "memory_usage_mb": float(df.memory_usage(deep=True).sum() / 1024 / 1024),
            "dtypes_distribution": {str(k): int(v) for k, v in df.dtypes.value_counts().to_dict().items()},
            "missing_data_summary": {
                "total_missing": int(df.isnull().sum().sum()),
                "columns_with_missing": int((df.isnull().sum() > 0).sum()),
                "rows_with_missing": int(df.isnull().any(axis=1).sum())
            }
        }
        
        # –ß–∏—Å–ª–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            numeric_desc = df[numeric_cols].describe()
            stats["numeric_summary"] = {
                str(col): {str(stat): float(val) for stat, val in numeric_desc[col].items()}
                for col in numeric_desc.columns
            }
        
        return stats

class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ Ollama"""
    
    def __init__(self, model_name: str, logger: Logger):
        self.model_name = model_name
        self.logger = logger
    
    def analyze_data(self, filename: str, analysis_data: Dict[str, Any]) -> str:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ LLM"""
        prompt = self._build_prompt(filename, analysis_data)
        
        self.logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ {self.model_name}...")
        start_time = time.time()
        
        try:
            response = self._call_ollama(prompt)
            elapsed = time.time() - start_time
            self.logger.success(f"–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.1f}—Å")
            return response
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ LLM: {e}")
            raise
    
    def _build_prompt(self, filename: str, analysis_data: Dict[str, Any]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"""
        def safe_truncate(obj, max_chars=4000):
            s = safe_json_dumps(obj, ensure_ascii=False, indent=2)
            if len(s) > max_chars:
                return s[:max_chars] + "... [truncated]"
            return s
        
        overview = analysis_data.get("overview", {})
        sample = analysis_data.get("sample", {})
        stats = analysis_data.get("statistics", {})
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
        separator_info = overview.get("separator_info", {})
        separator_text = ""
        if separator_info:
            sep_name = separator_info.get("separator_name", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            expected_cols = separator_info.get("expected_columns", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            separator_text = f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ: {sep_name}, –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç–æ–ª–±—Ü–æ–≤: {expected_cols}"
        
        prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.

–§–∞–π–ª: {filename}{separator_text}

–°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–•:
{safe_truncate(overview)}

–ü–†–ò–ú–ï–†–´ –î–ê–ù–ù–´–•:
{safe_truncate(sample)}

–°–¢–ê–¢–ò–°–¢–ò–ö–ê:
{safe_truncate(stats)}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å:
- –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∞–Ω–Ω—ã—Ö (—á—Ç–æ —ç—Ç–æ –∑–∞ –¥–∞–Ω–Ω—ã–µ, –æ—Ç–∫—É–¥–∞ –º–æ–≥—É—Ç –±—ã—Ç—å)
- –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö  
- –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–ø—É—Å–∫–∏, –≤—ã–±—Ä–æ—Å—ã, –¥—É–±–ª–∏–∫–∞—Ç—ã)
- –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–ª–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∞–Ω–∞–ª–∏–∑—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–∏—Å–∫–∏
- –ë–∏–∑–Ω–µ—Å-–≤—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö
- –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Å—Ç–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è 
(–Ω–∞–ø—Ä–∏–º–µ—Ä: –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ ‚Äì –≤ ClickHouse, —Å—ã—Ä—ã–µ ‚Äì –≤ HDFS, 
–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ ‚Äì –≤ PostgreSQL)

–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
"""
        return textwrap.dedent(prompt).strip()
    
    def _call_ollama(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ Ollama API"""
        # –ü—Ä–æ–±—É–µ–º Python –∫–ª–∏–µ–Ω—Ç
        if OLLAMA_CLIENT_OK:
            try:
                response = ollama.chat(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.get("message", {}).get("content", "").strip()
            except Exception as e:
                self.logger.warning(f"Python –∫–ª–∏–µ–Ω—Ç Ollama –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}, –ø—Ä–æ–±—É–µ–º CLI...")
        
        # –§–æ–ª–±—ç–∫ –Ω–∞ CLI
        try:
            result = subprocess.run(
                ["ollama", "run", self.model_name],
                input=prompt.encode("utf-8"),
                capture_output=True,
                check=True
            )
            return result.stdout.decode("utf-8").strip()
        except subprocess.CalledProcessError as e:
            error_msg = e.stderr.decode("utf-8", errors="ignore")
            raise RuntimeError(f"–û—à–∏–±–∫–∞ Ollama CLI: {error_msg}")

class ResultSaver:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, output_dir: str, logger: Logger):
        self.output_dir = Path(output_dir)
        self.logger = logger
        self.output_dir.mkdir(exist_ok=True)
    
    def save_analysis(self, filename: str, analysis_data: Dict[str, Any], llm_response: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = Path(filename).stem
        
        # JSON —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        json_path = self.output_dir / f"{base_name}_analysis_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            f.write(safe_json_dumps({
                "filename": filename,
                "analysis_data": analysis_data,
                "llm_response": llm_response,
                "timestamp": timestamp
            }, ensure_ascii=False, indent=2))
        
        # Markdown –æ—Ç—á–µ—Ç
        md_path = self.output_dir / f"{base_name}_report_{timestamp}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(self._create_markdown_report(filename, analysis_data, llm_response))
        
        self.logger.success(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}, {md_path}")
        return str(md_path)
    
    def _create_markdown_report(self, filename: str, analysis_data: Dict[str, Any], llm_response: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ Markdown –æ—Ç—á–µ—Ç–∞"""
        overview = analysis_data.get("overview", {})
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ
        separator_section = ""
        separator_info = overview.get("separator_info", {})
        if separator_info:
            separator_section = f"""
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä—Å–∏–Ω–≥–µ

- **–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å:** {separator_info.get("separator_name", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")}
- **–û–∂–∏–¥–∞–µ–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤:** {separator_info.get("expected_columns", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")}
- **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å:** {separator_info.get("consistency", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")}
- **–ö–æ–¥–∏—Ä–æ–≤–∫–∞:** {separator_info.get("encoding", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")}
"""
        
        report = f"""# –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö: {filename}

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## –û–±–∑–æ—Ä —Ñ–∞–π–ª–∞

- **–§–æ—Ä–º–∞—Ç:** {overview.get('data_type', 'unknown')}
- **–°—Ç—Ä–æ–∫:** {overview.get('rows', 'unknown'):,}
- **–°—Ç–æ–ª–±—Ü–æ–≤:** {overview.get('cols', 'unknown')}
- **–†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏:** {overview.get('memory_usage_mb', 0):.2f} MB{separator_section}

## –ê–Ω–∞–ª–∏–∑ –æ—Ç LLM

{llm_response}

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

```json
{safe_json_dumps(overview, ensure_ascii=False, indent=2)}
```

## –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö

```json
{safe_json_dumps(analysis_data.get('sample', [])[:5], ensure_ascii=False, indent=2)}
```

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

```json
{safe_json_dumps(analysis_data.get('statistics', {}), ensure_ascii=False, indent=2)}
```
"""
        return report

def create_argument_parser() -> argparse.ArgumentParser:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM –∏ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è CSV",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent("""
        –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
          %(prog)s                              # –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ ./input/
          %(prog)s -i data/ -o results/         # —É–∫–∞–∑–∞—Ç—å –ø–∞–ø–∫–∏
          %(prog)s -f myfile.csv                # –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª
          %(prog)s -m llama2 -s 20              # –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å –∏ –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤
          %(prog)s --force-separator ";"        # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
          %(prog)s -c 27 -v                     # –æ–∂–∏–¥–∞–µ—Ç—Å—è 27 —Å—Ç–æ–ª–±—Ü–æ–≤, –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–µ–∂–∏–º
        """)
    )
    
    parser.add_argument("-i", "--input-dir", default=DEFAULT_INPUT_DIR,
                       help=f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_INPUT_DIR})")
    parser.add_argument("-o", "--output-dir", 
                       help=f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_OUTPUT_DIR})")
    parser.add_argument("-f", "--file-pattern",
                       help="–ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –≤ –∏–º–µ–Ω–∏)")
    parser.add_argument("-m", "--model", default=DEFAULT_MODEL,
                       help=f"–ú–æ–¥–µ–ª—å Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_MODEL})")
    parser.add_argument("-s", "--sample-rows", type=int, default=10,
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç—Ä–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)")
    parser.add_argument("-c", "--expected-cols", type=int,
                       help="–û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ (–¥–ª—è CSV)")
    parser.add_argument("--force-separator", 
                       help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: ';' –∏–ª–∏ '\\t')")
    parser.add_argument("--max-chars", type=int, default=4000,
                       help="–ú–∞–∫—Å–∏–º—É–º —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4000)")
    parser.add_argument("--no-save", action="store_true",
                       help="–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã")
    parser.add_argument("-v", "--verbose", action="store_true",
                       help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π")
    
    return parser

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = AnalysisConfig(
        input_dir=args.input_dir,
        output_dir=args.output_dir or (DEFAULT_OUTPUT_DIR if not args.no_save else None),
        model_name=args.model,
        sample_rows=args.sample_rows,
        max_chars=args.max_chars,
        expected_cols=args.expected_cols,
        file_pattern=args.file_pattern,
        save_results=not args.no_save,
        verbose=args.verbose,
        force_separator=args.force_separator
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    analyzer = DataAnalyzer(config)
    llm_client = LLMClient(config.model_name, analyzer.logger)
    result_saver = ResultSaver(config.output_dir, analyzer.logger) if config.save_results else None
    
    try:
        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
        files = analyzer.file_handler.find_files(config.input_dir, config.file_pattern)
        if not files:
            analyzer.logger.error(f"–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {config.input_dir}")
            sys.exit(1)
        
        analyzer.logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        file_to_analyze = files[0]
        analyzer.logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {file_to_analyze.path}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        analysis_data = analyzer.analyze_file(file_to_analyze)
        
        # LLM –∞–Ω–∞–ª–∏–∑
        llm_response = llm_client.analyze_data(
            os.path.basename(file_to_analyze.path), 
            analysis_data
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*80)
        print("–ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê")
        print("="*80)
        overview = analysis_data.get("overview", {})
        print(f"üìä –§–∞–π–ª: {file_to_analyze.path}")
        print(f"üìã –§–æ—Ä–º–∞—Ç: {overview.get('data_type', 'unknown')}")
        print(f"üìè –†–∞–∑–º–µ—Ä: {overview.get('rows', 0):,} —Å—Ç—Ä–æ–∫ √ó {overview.get('cols', 0)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        print(f"üíæ –ü–∞–º—è—Ç—å: {overview.get('memory_usage_mb', 0):.2f} MB")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ
        separator_info = overview.get("separator_info", {})
        if separator_info:
            sep_name = separator_info.get("separator_name", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            print(f"üîç –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: {sep_name}")
        
        print("\n" + "="*80)
        print("–°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–•")
        print("="*80)
        print(safe_json_dumps(overview, ensure_ascii=False, indent=2))
        
        print("\n" + "="*80)
        print("–ê–ù–ê–õ–ò–ó LLM")
        print("="*80)
        print(llm_response)
        
        if config.verbose:
            print("\n" + "="*80)
            print("–ü–û–ü–´–¢–ö–ò –ß–¢–ï–ù–ò–Ø (–ø–µ—Ä–≤—ã–µ 5)")
            print("="*80)
            attempts = overview.get("read_attempts", [])
            for i, (cfg, status) in enumerate(attempts[:5], 1):
                print(f"{i:02d}. {cfg} -> {status}")
            
            print("\n" + "="*80) 
            print("–ü–†–ò–ú–ï–†–´ –î–ê–ù–ù–´–• (–ø–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏)")
            print("="*80)
            sample = analysis_data.get("sample", [])
            print(safe_json_dumps(sample[:3], ensure_ascii=False, indent=2))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if result_saver:
            report_path = result_saver.save_analysis(
                file_to_analyze.path,
                analysis_data, 
                llm_response
            )
            print(f"\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
        analyzer.logger.success("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        analyzer.logger.warning("–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        analyzer.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        if config.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
