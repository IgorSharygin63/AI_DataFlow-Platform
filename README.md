# AI_DataFlow-Platform
Репозиторий создан в рамках хакатона ЛЦТ2025 для задачи Интеллектуальный цифровой инженер данных

# Инструкция по использованию универсального анализатора данных ( каталог LDT)

Этот документ описывает, как использовать скрипт `universal_data_analyzer.py` для автоматического анализа и получения инсайтов из ваших данных с помощью локальных языковых моделей (LLM) через Ollama.

## 1. Обзор

Скрипт представляет собой инструмент командной строки, который:
1.  **Находит файлы** в указанной директории (input).
2.  **Автоматически определяет формат** данных (CSV, JSON, XML, Excel, Parquet).
3.  **Надежно считывает данные**, особенно CSV, с автоматическим определением разделителя (`,`, `;`, `\t`, `|`) и кодировки.
4.  **Собирает метаданные** и структурную информацию о файле.
5.  **Формирует промпт** для языковой модели, включающий структуру, статистику и примеры данных.
6.  **Отправляет запрос** в локально запущенную LLM через Ollama.
7.  **Выводит в консоль и сохраняет** детальный отчет в форматах Markdown и JSON, включающий как технический анализ, так и выводы от LLM.

## 2. Требования и установка

### Системные требования
- Python 3.x
- Установленный и запущенный [Ollama](https://ollama.com/) с загруженной моделью (например, `qwen3:30b`).

### Установка зависимостей Python
Скрипт требует несколько Python-библиотек. Установите их с помощью `pip`:

**Обязательные:**
```bash
pip install pandas lxml ollama
```

**Опциональные (рекомендуются для полной функциональности):**
```bash
pip install openpyxl pyarrow rich
```
- `openpyxl`: для работы с файлами Excel (`.xlsx`, `.xls`).
- `pyarrow`: для работы с файлами Parquet.
- `rich`: для красивого и информативного вывода в консоли.

## 3. Подготовка к работе

1.  **Запустите Ollama**: Убедитесь, что сервис Ollama активен и модель, которую вы планируете использовать, загружена. Вы можете загрузить модель командой:
    ```bash
    ollama pull qwen3:30b 
    ```
    *(`qwen3:30b` — модель по умолчанию, вы можете использовать любую другую).*

2.  **Создайте структуру папок**: По умолчанию скрипт ищет файлы в папке `input` и сохраняет результаты в `output`. Создайте их рядом со скриптом:
    ```
    /your_project_folder
    |-- universal_data_analyzer.py  (ваш скрипт)
    |-- input/
    |   `-- my_data.csv
    `-- output/
    ```

3.  **Поместите ваши данные**: Скопируйте файл(ы) для анализа (например, `my_data.csv`) в папку `input`.

## 4. Использование

Скрипт запускается из командной строки. Ниже приведены основные сценарии использования.

### 4.1. Базовый запуск

Для анализа первого найденного файла в папке `input` с использованием модели по умолчанию (`qwen3:30b`), просто запустите скрипт без аргументов:

```bash
python3 universal_data_analyzer.py
```

Скрипт проанализирует файл, выведет краткую сводку и анализ от LLM в консоль, а также создаст два файла с отчетом в папке `output`:
- `my_data_report_YYYYMMDD_HHMMSS.md` (Отчет в формате Markdown)
- `my_data_analysis_YYYYMMDD_HHMMSS.json` (Полные данные анализа в JSON)

### 4.2. Основные аргументы

Вы можете кастомизировать работу скрипта с помощью аргументов командной строки:

| Аргумент              | Сокращение | Описание                                                                  | Пример                                    |
|-----------------------|------------|---------------------------------------------------------------------------|-------------------------------------------|
| `--input-dir`         | `-i`       | Указать папку для поиска данных.                                          | `-i /path/to/data`                        |
| `--output-dir`        | `-o`       | Указать папку для сохранения отчетов.                                     | `-o ./reports`                            |
| `--file-pattern`      | `-f`       | Анализировать конкретный файл (по подстроке в имени).                     | `-f orders_2024.csv`                      |
| `--model`             | `-m`       | Указать имя модели Ollama для анализа.                                    | `-m llama3`                               |
| `--no-save`           |            | Не сохранять результаты в файлы, только выводить в консоль.               | `--no-save`                               |
| `--verbose`           | `-v`       | Включить подробный вывод с диагностикой (попытки чтения, кодировки и т.д.).| `-v`                                      |

**Пример с аргументами:**
```bash
python3 universal_data_analyzer.py -i ./data -o ./results -m llama3 -v
```

### 4.3. Работа с CSV файлами

Скрипт особенно эффективен для анализа "проблемных" CSV файлов.

#### Принудительное указание разделителя
Если автоопределение разделителя работает некорректно, вы можете указать его вручную.

- **Для точки с запятой (`;`):**
  ```bash
  python3 universal_data_analyzer.py --force-separator ";"
  ```
- **Для табуляции (`\t`):** (важно использовать кавычки)
  ```bash
  python3 universal_data_analyzer.py --force-separator "\t"
  ```

#### Помощь в парсинге
Если вы знаете, сколько столбцов должно быть в вашем CSV, это поможет скрипту выбрать наилучший способ парсинга.

```bash
# Если вы ожидаете, что в файле 27 колонок
python3 universal_data_analyzer.py -c 27
```

## 5. Пример вывода

После успешного выполнения в консоли появится отчет, разделенный на блоки:

1.  **КРАТКАЯ СВОДКА**: Основная информация о файле (формат, размер, количество строк и столбцов).
2.  **СТРУКТУРА ДАННЫХ**: Детальная информация о каждом столбце в формате JSON (тип данных, % пропусков, примеры значений).
3.  **АНАЛИЗ LLM**: Развернутый текстовый отчет от языковой модели, который включает:
    - Резюме и бизнес-контекст данных.
    - Оценку качества данных.
  

# Инструкция по запуску интерактивного чат-агента

Этот документ объясняет, как настроить и запустить скрипт для интерактивного чата с AI-агентом, который использует внешние инструменты через протокол MCP (Multi-Server Control Protocol).

## 1. Обзор

Скрипт выполняет следующие функции:
1.  **Конфигурирует и запускает** один или несколько "серверов-инструментов" в виде отдельных Python-процессов.
2.  **Использует `langchain_mcp_adapters`** для управления этими серверами и загрузки предоставляемых ими инструментов (например, `web_search`, `db_query`).
3.  **Инициализирует большую языковую модель (LLM)**, работающую локально через Ollama (например, `qwen3:30b`).
4.  **Создает AI-агента** с помощью `LangGraph`, который способен понимать запрос пользователя и выбирать подходящий инструмент для его выполнения.
5.  **Запускает интерактивный чат** в командной строке, позволяя пользователю общаться с агентом и использовать его возможности.

## 2. Требования и установка

### Системные требования
- Python 3.x
- Установленный и запущенный [Ollama](https://ollama.com/) с загруженной моделью (например, `qwen3:30b`).

### Установка зависимостей Python
Скрипт требует несколько Python-библиотек. Установите их с помощью `pip`:

```bash
pip install langchain-mcp-adapters langgraph langchain-ollama
```
Возможно, потребуются и другие зависимости `langchain`, в зависимости от вашей среды.

## 3. Подготовка к работе

### 3.1. Настройка путей к серверам
**Это самый важный шаг.** Внутри скрипта вам необходимо правильно указать пути к файлам, которые реализуют ваши серверы-инструменты.

Откройте скрипт и найдите секцию `server_paths`:
```python
# Пути к серверам
server_paths = {
    "search": r"E:\agent_mcp-main\mcp_server\search_sever_duckduck_go.py",
    "db_sever": r"E:\agent_mcp-main\sqlite_explorer_v2.py"
}
```
- **Замените пути** на актуальные пути к вашим файлам серверов.
- **Ключи словаря** (`"search"`, `"db_sever"`) — это внутренние имена, которые вы даете серверам. Они могут быть любыми.

Найдите и укажите путь к корневой директории вашего проекта в переменной `project_root`:
```python
# Конфигурация серверов для MCP клиента
project_root = r"E:\agent_mcp-main"
```
Этот путь будет добавлен в `PYTHONPATH` для дочерних процессов, чтобы они могли найти все необходимые модули вашего проекта.

### 3.2. Запуск Ollama
Убедитесь, что сервис Ollama запущен и модель, указанная в скрипте (`qwen3:30b` по умолчанию), загружена.
```bash
ollama pull qwen3:30b
```

## 4. Запуск скрипта

После того как все пути настроены и зависимости установлены, запустите основной скрипт из командной строки:

```bash
python3 your_script_name.py
```

При успешном запуске вы увидите приветственное сообщение:

```
✅ Серверы готовы к работе: ['search', 'db_sever']
✅ Загружено инструментов: 5
======================================================================
🚀 ИНТЕРАКТИВНЫЙ ЧАТ УСПЕШНО ЗАПУЩЕН!
📊 Активные серверы: search, db_sever
🛠️ Доступно инструментов: 5
💡 Команды: 'exit', 'quit', 'выход' - для завершения
💡 'tools' - показать доступные инструменты
======================================================================
```

## 5. Взаимодействие с агентом

Теперь вы можете общаться с агентом прямо в терминале.

### Команды чата

- **Задать вопрос агенту**: Просто введите ваш вопрос и нажмите Enter.
  > `🤔 Вы: найди информацию о последнем альбоме группы Tool`
- **Посмотреть доступные инструменты**: Введите команду `tools`.
  > `🤔 Вы: tools`
  >
  > `🤖 Доступные инструменты:`
  > `1. search`
  > `2. run_sql`
  > `3. list_tables`
  > `...`
- **Выйти из чата**: Введите `exit`, `quit` или `выход`.

### Логирование

Скрипт ведет лог ошибок в файл `mcp_debug.log`. Если вы столкнулись с проблемами при загрузке инструментов или в работе агента, проверьте этот файл для получения детальной информации и трассировки ошибок.

    - Найденные аномалии и паттерны.
    - Рекомендации по предобработке, анализу и хранению.
4.  **(В режиме `--verbose`)** **ПОПЫТКИ ЧТЕНИЯ** и **ПРИМЕРЫ ДАННЫХ**: Дополнительная отладочная информация.

Готово! Теперь вы можете использовать этот инструмент для быстрой и глубокой аналитики ваших наборов данных.
